{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied policy gradient method on control environments. Would suggest reading this after the DQN notebook since that was the order I learned these strategies too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import one_hot, log_softmax, softmax, normalize\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of assistance in setting this up via Pytorch from \n",
    "\n",
    "https://github.com/halahup/RL-Primers/blob/master/Policy%20Gradient/policy_gradient.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize environment, device, and hyperparameters\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "ALPHA = 0.001 #learning rate\n",
    "GAMMA = 0.98 #discounting for future events\n",
    "BETA  = 0. #amount to add to entropy bonus on objective function  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: CartPole is one of the games which is one that depends a lot less on a brute force sort of search for the optimal policies, as opposed to a game like MountainCar which eventually finds its way to the goal and works its way back from there. Because of that, I'll save the video for CartPole since it will show the most information in the least amount of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network to approximate the policy function. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        \n",
    "        self.l1 = nn.Linear(self.state_space, 128, bias=False)\n",
    "        self.l2 = nn.Linear(128, self.action_space, bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1) \n",
    "        )  #output is log probs\n",
    "        return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discounted_rewards(rewards: np.array, gamma: float) -> np.array:\n",
    "    \"\"\"\n",
    "        Calculates the sequence of discounted rewards-to-go.\n",
    "        Args:\n",
    "            rewards: the sequence of observed rewards\n",
    "            gamma: the discount factor\n",
    "        Returns:\n",
    "            discounted_rewards: the sequence of the rewards-to-go\n",
    "    \"\"\"\n",
    "    discounted_rewards = np.empty_like(rewards, dtype=np.float)\n",
    "    for i in range(rewards.shape[0]):\n",
    "        gammas = np.full(shape=(rewards[i:].shape[0]), fill_value=gamma)\n",
    "        discounted_gammas = np.power(gammas, np.arange(rewards[i:].shape[0]))\n",
    "        discounted_reward = np.sum(rewards[i:] * discounted_gammas)\n",
    "        discounted_rewards[i] = discounted_reward # returns dicounted rewards value at each step in rewards array.\n",
    "        \n",
    "\n",
    "        \n",
    "    return discounted_rewards\n",
    "\n",
    "\n",
    "def calculate_loss(epoch_logits: torch.Tensor, weighted_log_probs: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "        Calculates the policy \"loss\" and the entropy bonus\n",
    "        Args:\n",
    "            epoch_logits: logits of the policy network we have collected over m episodes\n",
    "            weighted_log_probs: loP * W of the actions taken\n",
    "        Returns:\n",
    "            policy loss + the entropy bonus\n",
    "            entropy: for logging\n",
    "    \"\"\"\n",
    "    policy_loss = -1 * torch.mean(weighted_log_probs) #expected value of log probability (policy) times rewards weight\n",
    "\n",
    "    # add the entropy bonus\n",
    "    p = softmax(episode_logits, dim=1)\n",
    "    log_p = log_softmax(episode_logits, dim=1)\n",
    "    entropy = -1 * torch.mean(torch.sum(p * log_p, dim=1), dim=0)\n",
    "    entropy_bonus = -1 * BETA * entropy\n",
    "\n",
    "    return policy_loss + entropy_bonus, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize policy network along with optimizer. \n",
    "agent = Policy().to(device)\n",
    "optimizer = optim.Adam(params= agent.parameters(), lr=ALPHA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last few parameters to \n",
    "RENDER = True #play a nice movie :-)\n",
    "epochs = 1000 #total number epochs. totally fine to stop before this though with interrupt. \n",
    "m = 10  #how many episodes per epoch. \n",
    "total_rewards = [] #for tracking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for recording videos of rendered results\n",
    "env = wrappers.Monitor(env, '../videos/Policy_Gradient/', force = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1dac09d2dadc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# get logits from model based on state input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0maction_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# append the logits to the episode logits list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-352595fd6dc5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         )  #output is log probs\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    805\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    806\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    \n",
    "    # reset this every m episodes\n",
    "    epoch_logits = torch.empty(size=(0, env.action_space.n), device=DEVICE)\n",
    "    epoch_weighted_log_probs = torch.empty(size=(0,), dtype=torch.float, device=DEVICE)\n",
    "\n",
    "    \n",
    "    \n",
    "    for episode in range(m): \n",
    "\n",
    "        state = env.reset()\n",
    "        # initialize arrays holding episodic info\n",
    "        episode_actions = torch.empty(size=(0,),\n",
    "                                      dtype=torch.long, device=DEVICE) #all actions taken\n",
    "        episode_logits = torch.empty(size=(0,\n",
    "                                           env.action_space.n), device=DEVICE) #all log probabilities\n",
    "        average_rewards = np.empty(shape=(0,),\n",
    "                                   dtype=np.float) #avg reward per step\n",
    "        episode_rewards = np.empty(shape=(0,), \n",
    "                                   dtype=np.float) #rewards accumulated in episode\n",
    "\n",
    "\n",
    "        done = False #initial system is not done. \n",
    "        \n",
    "        while not done: \n",
    "            if RENDER:\n",
    "                #every 5 episodes, render progress. \n",
    "                if episode % 5 == 0:\n",
    "                    env.render()\n",
    "\n",
    "            # get logits from model based on state input        \n",
    "            action_logits = agent(torch.tensor(state).float().unsqueeze(dim=0).to(DEVICE))\n",
    "\n",
    "            # append the logits to the episode logits list\n",
    "            episode_logits = torch.cat((episode_logits, action_logits), dim=0)\n",
    "\n",
    "            # sample an action according to the policy\n",
    "            action = Categorical(logits=action_logits).sample()\n",
    "\n",
    "            # append the action to the episode action list to obtain the trajectory\n",
    "            episode_actions = torch.cat((episode_actions, action), dim=0)\n",
    "\n",
    "            # take the chosen action, observe the reward and the next state\n",
    "            state, reward, done, _ = env.step(action=action.cpu().item())\n",
    "\n",
    "\n",
    "            # we need the rewards so we can calculate the weights for the policy gradient\n",
    "            episode_rewards = np.concatenate((episode_rewards, np.array([reward])), axis=0)\n",
    "\n",
    "            # here the average reward is state specific\n",
    "            average_rewards = np.concatenate((average_rewards,\n",
    "                                              np.expand_dims(np.mean(episode_rewards), axis=0)),\n",
    "                                             axis=0)\n",
    "\n",
    "\n",
    "        #now leave episode loop. \n",
    "\n",
    "\n",
    "        # turn the rewards we accumulated during the episode into the rewards-to-go:\n",
    "        # earlier actions are responsible for more rewards than the later taken actions\n",
    "        discounted_rewards_to_go = get_discounted_rewards(rewards=episode_rewards,\n",
    "                                                            gamma=GAMMA)\n",
    "\n",
    "       # print(discounted_rewards_to_go)\n",
    "        discounted_rewards_to_go -= average_rewards #what the baseline is! \n",
    "       # print(discounted_rewards_to_go)\n",
    "        # baseline = rewards to go -=  state specific average ABSOLUTE VALUE\n",
    "        #does this still make sense if it is negative at every step? Makes it 0\n",
    "\n",
    "\n",
    "        # # calculate the sum of the rewards for the running average metric\n",
    "        sum_of_rewards = np.sum(episode_rewards)\n",
    "\n",
    "        # set the mask for the actions taken in the episode\n",
    "        mask = one_hot(episode_actions, num_classes=env.action_space.n)\n",
    "\n",
    "        # calculate the log-probabilities of the taken actions\n",
    "        # mask is needed to filter out log-probabilities of logits which were not selected via the stochastic choice.\n",
    "        episode_log_probs = torch.sum(mask.float() * log_softmax(episode_logits, dim=1), dim=1)\n",
    "                #same as sum of weighted log probs\n",
    "            \n",
    "        # weight the episode log-probabilities by the rewards-to-go\n",
    "        episode_weighted_log_probs = episode_log_probs * \\\n",
    "            torch.tensor(discounted_rewards_to_go).float().to(DEVICE)\n",
    "\n",
    "        # append the logits - needed for the entropy bonus calculation\n",
    "        epoch_logits = torch.cat((epoch_logits, episode_logits), dim=0)\n",
    "\n",
    "        epoch_weighted_log_probs = torch.cat((epoch_weighted_log_probs, episode_weighted_log_probs),\n",
    "                                    dim=0)\n",
    "\n",
    "        # calculate the sum over trajectory of the weighted log-probabilities\n",
    "        sum_weighted_log_probs = torch.sum(episode_weighted_log_probs).unsqueeze(dim=0)\n",
    "\n",
    "\n",
    "        #here I would instead do this over m episodes as in the usual notes. \n",
    "        #loss would be a result of multiple\n",
    "\n",
    "        #m_ep_logits = torch.cat(m_ep_logits, episode_logits)\n",
    "        #m_ep_wlogpi = torch.cat(m_ep_wlogpi, episode_weighted_log_probs)\n",
    "\n",
    "        loss, entropy = calculate_loss(epoch_logits=epoch_logits,\n",
    "                                    weighted_log_probs=epoch_weighted_log_probs)\n",
    "\n",
    "\n",
    "        #then using this information, computer gradient and backprop!\n",
    "        #repeat until satisfied with performance in episode.\n",
    "\n",
    "        # zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # backprop\n",
    "        loss.backward(retain_graph=True )\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        total_rewards.append(sum_of_rewards)\n",
    "    \n",
    "    epoch_logits = torch.empty(size=(0, env.action_space.n), device=DEVICE)\n",
    "    epoch_weighted_log_probs = torch.empty(size=(0,), dtype=torch.float, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    \"\"\"\n",
    "    Useful function to smooth rewards over episodes. \n",
    "    \"\"\"\n",
    "    cumsum = np.cumsum(np\n",
    "                       .insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the total average reward over 50 episodes, this is the nice curve we obtain for a policy gradient based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2c1d16d0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD5CAYAAAA3Os7hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxb1ZXA8Z8kS95t2bGdxYnjLOQmJCGBELISwr4WKJQWpoUWWmgpbaGlQ0uhM0yXmU5bKAW6UJZCoQWGHdKy7xCyQRKyvux24iSO7VjetWv+eE+yZMuxLWuzcr6fTz4f6ek9vWNFOrq6795zTYFAACGEEMOfOdUBCCGEiA9J6EIIkSEkoQshRIaQhC6EEBlCEroQQmQISehCCJEhsgayk1JqHvC/mqYtVUrNBu4FfIALuErTtHql1LXANwEv8AtN05YlKmghhBC9mfobh66UugW4EujQNG2+Uuo94EZN09Yppb4JKODXwBvAiUAO8CFwoqZpriM9t9/vD/h8sY2Dt1hMxHpsIklcgyNxDY7ENTiZGpfVamkEyntuH0gLfSdwCfCYcf9yTdMOhB3vBE4CPjISuEsptQM4Dlh9pCf2+QI4HJ0D+wt6sNvzYj42kSSuwZG4BkfiGpxMjau8vLAm2vZ+E7qmac8qparD7h8AUEotBL4DLAHOBlrCDmsDivt7bovFhN2e199ufRxrjvnYRJK4BkfiGhyJa3COtrgG1Ifek1LqS8BtwPmapjUopVqBwrBdCgFHf88jLfTkkbgGR+IaHIlrcOLQQo+6fdAJXSn1FfSLn0s1TTtsbF4F/FIplQNkA9OAjbGFKoQQIhaDSuhKKQtwD1ALPKeUAnhP07T/VErdA3yAPhTyNk3TnPEOVgghRN8GlNA1TdsDzDfulvaxzwPAA/EJSwghxGDJxCIhhMgQktCFECJDSEIXQog4c3R6eFNrSPp5Yxq2KIQQom//+epWlu9uZvroQkYX5STtvNJCF0KIOGvu9ADQ1OFO6nkloQshRJwV51gBcHR5knpeSehCCBFnRTl6b3a0FrrX5+dP7+3E6fHF/byS0IUQIs7suXoLfXtDR6/HNh1s4643t7N+f2vczysJXQgh4sxvlCX/LErSdnr9ANgs8U+/ktCFECLOPEat8xant9djbiOhZ2fFP/3KsEUhhIiTQCBAXYsTt09P2l3u3v3kwcdsktCFECJ9Pb1uP795eyeVxfrY884oFz5dwRa6dLkIIUT62nSwDYC6Fr3YrMvrx+uPXGoumNAT0UKXhC6EEHGSZ7X02tZzeKJbWuhCCJH+8rN792J/6ZE17GzsHr6YyD50SehCCBEnNoup17ZD7W6WbaoP3ZcuFyGEGAaCyRogy9yd3AuyLRH7ZJlNEY/HiyR0IYSIk/CEfs60Ck47pgzoLtYFepdLIsaggyR0IYSIG6enO6Gfd2wF/3vhseTbLDy1dn9o9qjL609IdwtIQhdCiLhxertHtIwq1MeiB1vj+42hjE6Pj9woo2HiQRK6EELESXgLvazABsBPzjwGgHaXXgagw+2jIMpomHiQhC6EEHHSFTbmPNgKLzRK6bY6wxJ6jiR0IYRIa50eHxNG5PHQFbND2wqN1vj9y2v0fdw+8m2S0IUQIq11uH1Ul+Zx3Jii0LZgQg+W0u1we6XLRQgh0p3e+o684FkY1r3i8wf0fbLloqgQQqS1aAk935ZFaZ6+glGr00OH2xe1REA8SEIXQog4CAQCdLq95Nl6t75/eNpkAA60uuh0+0LdMPEmCV0IIeLA5fXjC0SvuDhhRB4AT3xaRwAoK8hOSAyS0IUQIg6au/Tp/dG6UyaX5TOpLI91+1oAGGGMUY83SehCCBEHf1u1F4C54+xRHy/Pz+ZgmwuAEfmS0IUQIm19uOswJ08spdroXumpxLgwClBeKF0uQgiRtro8PkYX5fT5eGfYgtFVJdGT/lBJQhdCiDjo9PjIOULRrWsXjgfgjCllmBNQCx0gMWNnhBDiKOL1+fH4AuTZ+m4jq4oCVt+8JKFxSAtdCCGGqMuospiosrgDJS10IYSIkT8QwO3102lUWUx1QpcWuhBCxOj+5TWcfM9HNHa4geiTipJpQC10pdQ84H81TVuqlJoMPAIEgI3ADZqm+ZVS/wmcD3iBmzRNW5WgmIUQIi08vKIWgLe3NQCQG2XafzL120JXSt0CPAgEx+PcBdyuadrJgAm4SCl1AnAKMA+4HPhDYsIVQoj0U29MGMq1prbTYyAt9J3AJcBjxv05wHvG7VeAswANeF3TtABQq5TKUkqVa5rWcKQntlhM2O2xjce0WMwxH5tIEtfgSFyDI3ENTiLjChiLPgN0ePWLohWl+QM6X6Li6jeha5r2rFKqOmyTyUjcAG1AMVAENIXtE9x+xITu8wVwODoHFXCQ3Z4X87GJJHENjsQ1OBLX4CQyrvDl5hpa9Ra6z+UZ0PmGGld5eWHU7bH8PvCH3S4EHECrcbvndiGEyEgdxqLPAM2d+kXR4TjKZa1Saqlx+1zgA+Aj4GyllFkpVQWYNU1rjFOMQgiRdtrDpvIfak+PhB7LOPSbgQeUUjZgC/CMpmk+pdQHwMfoXxI3xDFGIYRImcZ2Fx5/oFedlvAWetCwGLaoadoeYL5xexv6iJae+9wB3BG/0IQQIrV8/gDffXYjOxo7eOP6BdjDKibe9+GeXvtbLYmp0TJQMrFICCH68McP97CjsQOAvY6uiMc8Xn+v/U0mSehCiDT1yV4H8+56n8PGRb+jzWtbD4Vu93wNvP4A88eXJDukI5KELoTo0/0f7cEfgO0NHakOJSVmju4evNfU6Yl4rM3lpTCnu9f6kX+bnbS4+iLFuYQQfWowapQkqHx32mt3+VAVBWiH2mnqiGyhtzm9FOVk8aXjxwAwfXRRKkKMIAldCNGnNqc+kqPD5etnz8zk6PJQlm/jQE4Wh8MSeiAQoNXlpTA7ixtOnpDCCCNJl4sQok8Wo2ne6Tn6Ero/EKC+zUVJnpUReTZqm7twGq9Dl8ePzx+gKCe92sSS0IUQfTIbozbaj8IW+vq6Vpq7PMwfX4I9z8qqWgfXPrkegFan3p9emC0JXQjRh/o2F09+WhdR+On9nU1sO9SekniCfecd7t6TaDKZ1+fnuqf05D25PJ/Gdr1Wy9ZD7bQ5vbQZk4oKpYUuhOjLPe/t4s53drK+rpU3tAb2Obq4+YVNfPmxT/lo1+Gkx+MzvleOtha6o6t7REtpnpVvL54Q6n5qcXpoNa4tpFsLPb2iEeIoV5pvA+DBFTWsrImsb3fT8xs5/4SxSYtlfV1LaGRHXUsXc+98n1tOn8xls8ckLYZkCwQC/PRfWxlZ2D3NvyjHyhmqnOwsMz94YRMtXR7ajRa69KELIfoUTBA9k3kq/OCFTaHbmw+2Afq49KAdDR08vKI2ontouPtsfyuvbW3gb6v3hrYFW+bFufq0f4fT291CT7OEnl7RCHGU8/h6TydPFUvYNPYDRr1vlzHd/bP9rXz9iXUAnHpMGSUl+ckPMAEa2iPHml88c1Tott1I6C1dnlAfelG2lXQiLXQh0ojbe+TW7vNr65IUCUQrS+I0EvqPXtoc2rauriVZISWcq0d9lh+dPjl0O/jrqdVooZuA/OzUVlfsSRK6EGnE64/eQp9frdcMueW5DaFt//hkHy9vPJiwWMYU6/3Is8ZEzoD8bH9rxMzR5h5T4oczl7f74u/FM0eRZelOkfnGAtCdbh9tTn3avznFxbh6ki4XIdKIu48ul88fN5oVe5qZU2UPbfvdu7sA+NyMUVGPGSqfP8DCCSUsmjCC9ftbQ9t/9qqGP+yHRCYV7nKGtdD9Pa4NWC1mbBYTfzKuIwS/8NKJtNCFSCNuX4DSsJrbFQU2LpoxitOOKWNMUTYjjUUW4tnXHpze31Or00tRjpWCHt0KNc1dNHa4uXZBFePsORnWQvdHvR2UZe5OmUVpNmQRpIUuRFrx+vwUZGeRk2VmrD2Xe78wM/SzvjjXGlolJ3iREuClDQc5f/rI0GiMgXJ6fJx8z0cAPHTFbL7+xDpuOmUiXz5RHxrZ5vJSlJ1Fvi16mqgoyMaea8uoFnp4Ei/K6X3BM7wEQrqNcAFJ6EKkFbcvQJbZxBNXzSXLYoroo82zWegw1rHcF7bYws9f30aWxcR5x44c1LkOhY3oeHubvgTw3e/twmI2Mboox2ihZ4VW4SnItkRMMKoozEZV5LNsUz1d7syYeOT0+MnOMvO9JRM4f/qRX8+KAluSoho46XIRIk3UNnfxzvZG2l1ebFnmXhfc8qyWUAu9rsUZ8VhHDAk1vL8+/Aviznd28sMX9THohTlZVBRkA/D9pZMijq8ozOacaRU4vX6++JcVNLS7GO5cXh85WWa+eHxln79MgiaMSL+hmpLQhUgDjR1uLn14NRDZcg4X3kI/0COhxzK3J3yR44aOvrtNJpfn89r187mwx8XXcfZcZlUWc+ms0Wytb+OzsAunw5XLq7fQB2JSWV6Coxk8SehCpIE1tf3PDC3MzqKpw4XX5+/VIm/pGvyFyfaw56jrsV5mkNvoUy7N07sXZlfqQxjfuH5BKPFdM68KgB+/vAW314+jy9PnhdZ01+72kWPte2z5E1+dE7o9u7I4GSENivShC5EGguVYj2Tu+BKeWX+Azw600tWjPvm+lugJ+UjCW+gtURJwWb6Ny46PrNvyu8/PoLHDjT1sJE5wSjzAixsP8uu3dpBrNfP+9xYPOqZEONzpxuMLMLIwu9djXr9+zeKx1Xt5fM0+Oty+0JdWNJPL8nnhG3PZ09RFQRqOcpEWuhBpoHUALdqJpfpP/ENt7ojx0gCvb22gc5D96O397P8f50zp1Y9ckJ1FdWlkV0N4F8Wv39oBdC8AkQ7O/tMKLvjLyl7b19Q6WPC7D9ha38Y97+/mcKcHl9fPudOOfDG0sjiXRRNLExXukKTfV4wQR6E2l5c8q4UfnzmZ4ijD5QCKc/WP699W7+21aLPXH+CUez9i1pgiHrxiYIsVB7tpvnLiWB5fs6/X40MdZ3240015Qe9WcbJsOtDK/p19lxx+dJVegGtzfTv2XGuoZG60lvxwIS10IdJAqzGV/NxpI1k4IXrrr9BI9MFkHpyAFD7+fP3+1lD1w5U1zUdsJe9u6qSiwBaRwJ7+2omhmiXRxmH35d/PmtJrW5cntYXGrnliHT95YWPofs3hztBr89CKGlbUNAPw2Oq9TC7vHrFSnobDEQdKEroQaaCly9Nvbe0ssymihkqw9ZtntYTGioP+5bCqppnvPLMh1AqNZldTJxPL8iPOm2M18/iVJ/DDUycx1j7wqe3XnTyRM6aURWzr2c+fbD2/y77w1zW8trUBgD9/VBPavs/hDF2UNiEtdCHEEDg9PtbsdTC1oqDffcOTVJmxGMb00YW88s353HqGXhmwod0dKu+68UDfQwmbO92U59siLmrac62MLsrhSydUYhpk4ame4+adKU7o0VraR3o9lkwawYofnHzEUS7pThK6ECnW1Ommy+Pn+LH9D4N78Mo5TBupJ/6inCwevHwW/3PBNIpzrSjjC2FXU0doCnubq++LrS1GrZYso9k/t8o+pGR2wYzIi4nJbqE3trsIBAL4AwEeWF4TtcZMcELWmCK9Ff7txdWhx3KtvSdzDTeS0IVIseB0+oEMgztlSnloir/PH2BWZXHoODWykMLsLFbVOCLWxIzG7fXj8vopyslidmUxF80cxR3nqCH9HQuqS/nndfP4/tKJQGL60PtaHelgq5Nz71/Jz1/bxo6GDv7ycQ3eKNcPgkvHYTJx7rQKrp5XFSqLm2cbvi3zIEnoQqRYh1tPMj2rGvYlOGa9Z19vltnE5PJ8apo7Qwk9O8tMS5eHrfVtkc8Rtmp9dpaZ28+aQkUc+o4rCrNZZFzUjWcLPRAIcOvLmzntD8ujflkdNIqVvbypHl9Y0v/FRdO58+LpofvBCVkenx+bUes8uNBznnX4D/qThC5EigVb6P3VDgmaaSw4cebU8l6PjSnOYX+LM5T0vP4ANz2/kSsfX0tjWK2Vfc36RKTiBFQMzDW6beLZh17b3MWb2xppd/n4/Xu7QotXB4Un+Z2N3UM6p48uCrXAoXtxig6XL3Qh2WaMow8OCx3OJKELkWLBFnr+AH/yL6gu5aMbFzNtZGGvxyqLcjjU7g6tjenxBdhS3w7AufevZJOx2POLxkpHlfbcIcffUzChx7PLJTxhL9tUzx2vaoBeA+eFzw5EPP5fr24D4PQpZUwfU0RB2Bfl4U43p/1hOZ0eXyiRB4d22nPTa33QWEhCFyLFOoIt9EFM5LH1UUAqOLJjl9FK9fj8lIQlql+9sR2Apg43E0rzmD6q95fCUOXZ9GGUGw+08uqWQzE/z8YDrfzf2jq8Pj9Pr9sf8diKPc3c/9Eebn5hE798Yzs7mzp7HX/9ompMJlPEup/hXzLBLpdgS70kAxL68P+NIcQw5vMHeOJTfeHngjhclBthDGXcb/Qpe3wBxtlzaOxwU15gY19LF15/gI/3NLNk0oghny8ai9lEWb6NN7c18ua2Rj7Z6+CHp00ecBVD0PvMr/7HOgAOd3pC48dPrLLj9wf4dF8LD66oDf2qOdTWu3Rv8AuyoI+urGBC12fmdpFjHf7t2+H/FwgxjL21rYFaoz87HuOfgwk9yOPzY7GYmTWmiM/PHE27y8dz6w8ARx7SOFThI0Ze2HCQZZsGt5h1TXN3sbGHVtSGbv/6c8dyyXGjQ/dD5YRbnRFL90H3F2R+HxebA+hdLRfO1MsCVxbHv/sp2SShC5Ei7+1o5FkjuWYNcvm4vvRM6PtbnWw52EaezRK66PfODn11om/Mr4rLOaP52blTOTvsom2dw3mEvXsLv7AZLj/bQnlh7wlD+1uclBdkc+MpE0Pbgr8IrJbuNPebC4/lJmOfNqOr68IZo3jnOwsZVzL8E3pMXS5KKSvwKFAN+IBrAS/wCBAANgI3aJqW2mIOQqSxH764OXT75evmxeU5e7ZSPb4AHp+PdpcvVPRrTa2DM6aUc9L4kricM5opFQV85+QJoa6S1kH+GnD2uKD6r2/Oo8XpxWwyRb142eL0MnVkFl86fgw1hzuZM84eMdP1e0smcNyYImZVFvPSBv3XQltYyeJ0LIUbi1hb6OcBWZqmLQR+BvwSuAu4XdO0k9FLIlwUnxCFyGzHjSkKTeMfKqvFTG6UvuANB1ojEuEJ4xK/OMOoohyeu2Yu4+w53RN6BqjnGPbygmwml+kFtCaOyOf6RdW9jrHnWrFazNx21hTOmVYR8diVc8cxy1iQosAYqtnmyox1UMPFmtC3AVlKKTNQBHiAOcB7xuOvAGcMPTwhMl+8f+oXRmlt/vTsKRxT0V1R8IQBlBmIh3EludhzbYNewahnvfeevthj4Q1gwKV6F1SXcFKVne+cPGFQMQ0Hsf7OaEfvbtkKlAEXAEs0TQtO0WoD+n3HWCwm7PbY1uWzWMwxH5tIEtfgSFxwQnXpgM81kLiKcm0caneTn22hw+XjolljuGqx3m+8cOIIMMEJk8oGXXwr1rhKC7M53OEa3OtpiWxr9jy2OEoJgNlRXsdocdmBv187f+CxJECi3l+xJvTvA69pmnarUmoc8DYQ/puxEOh3kUSfL4DD0Xv86EDY7XkxH5tIEtfgHK1xBWuSlORaOX9K2YDPNZC4gpV08616QreZCB3z+8/r0+BbYliyLta4csywoa6VO17YyBVzKgdUntbR7iLLbArVY4n23L+/ZAbjS3O58dmNlORZWTS2qNd+mfr+Ki+PPn8g1i6XZqDFuH0YsAJrlVJLjW3nAh/E+NxCZDy3T09UV8ypjFigIh6CE2WC47BTXXTq9Cn6aJe/f7KPO17ZOqBjnB5faMZpXxZOKKWyOJdnrpnLA5fPJssig/ZibaH/DnhYKfUBesv8J8Aa4AGllA3YAjwTnxCFyDyPrdYXnkhEudbgEEivT++HjnaRNJlOmdw9gam/vvHw/XKsZmZVllKRwmXshpuYErqmae3AF6M8dMrQwhHi6PCiMXQuEcWxerZUTxxnj/s5BiP8Syt8vL3H548YIx7O6fGRk2Xmd5+fkfD4Mon8RhEiBSaV5ZOdZeaCGaPi/tzBpHntwvH89qLpoeF66SAY2yd7HSy8+0M2RVlB6BevbeO1rQ1kZw3/+uTJlhmj6YUYZg61uzipyh63GaLhgs+ZZ7VEdHekksUEvgCs2dvCrS9v5s1t+mzVZZvqmT66KGLfYCXI8aXDf+ZmskkLXYgUaGh3x2VBiWiCFxOjLNiTMvd/aVbodjCZg154qy8zeyR60T9poQuRZH9ZvgdHlyfqIsbx8MPTJlGaZ+XkiaUJef5Y9NXt09AeWSUxeCEX4j/h6mggCV2IJGpzenngY7164EBnNg5WaZ6NH542OSHPPRR//uJxrKl18MleB1fOHcdb2xtZUxs5XSW8AuToIhndMliS0IVIovCENSIvMS30dDVnnJ05YSNu1tW19lpKrjWsRMBAJiCJSNKHLkQSBRP6yMJsjk9SPZV0lWcz4/UH8IR1s9z3wW4Avr90IkU5w38FoWSThC5EEgWrDt5xjkr5DM5U6157VK966PT4eHdHEwCXzupdfEv0TxK6EEkUrDoYrSLi0Savx2LSB41l5G4985hBLVcnusm7SogkCna5FOQc3a1z6G6h729x8qs3t4dWW6qW8ecxk4QuRBIFE7q00CHX6HK67qn1oW02i4mJpfl9HSL6Ib9rhEiiYB96fh8r0R9NohUNO2VyGfY8uRgaK0noQiRRm8tHvs0S95K5w1FelPK4MploaCShC5FEbU4PRQmosDgc9VyYubo0l/kJXLj6aCDvLCESzOnxcajdjc1i4p+bD6U6nLQxvjRyCbanr56bokgyh7TQhUiwr/1jLZc+vDo0xlp0++1FxwJww+Lq1AaSIaSFLkSC7WzU1468852dAHx5zthUhpNWTplcxuqbl6Q6jIwhLXQhkmh8SS43LZ2Y6jBEhpIWuhAJ5DLW0Mwym/jS8ZXceMqEFEckMpm00EXGum3ZFs6/f0VKY2hz6gs43HzqJG5aOhFTAhaFFiJIWugiY72uNaQ6hNCKPDJUUSSDtNBFxgtfBQdg08E2nvq0Linnfme7vtzaxBEynV0kniR0kfEW3v0hf/poT+j+LS9u4rfv7KS+zdX3QXGyv9XJ6KJsJpdLQheJJwldZKRAoHuF5ADw8IraUALPN2YobtjfmvA4Ot0+qdsikkYSushI7S5fr23BBYnzjSp/Lc6+V5wH/Uvhvg92s7q2OeY4ujy+UJlYIRJNErrISIc79bUqr5k3jvOnjwTA0aUncKtFf9u3dHmjH2xYVevg0VV7uff93YM698qa5tC5Ot1+8mzyMRPJIe80kZG2N3QAcPzYYr4xvwrQE/pDK2pYu68ldP9Iapu7AAjrvemXx+fnO89s4Non1wHSQhfJJQldZKRbl20BoCTPhj1Xr699sNXFnz+qCe2z+3Bn6HZdSxcX/GUlT6/bH9p2yOhzH0Q+D9U733NY/zLo9PiO+rVDRfJIQhfDnqPLQ7PRxdLTiHwb+TYL+TYL9y+viXhsVVjXyNb6durbXDy2em/o8WCfe1/PHU1bWN/9/7yxnf0tTqxm+ZiJ5JB3mhj2blu2hbP+tCLUOgZ98YTJZfmU5dswmUzMqizqdZw/APsceku6oV1P2vVtLn715nacHh/1xramDnevsex9CY/huc8OAODxD+xYIYZKEroY9lbVOgB4e3sjTo+PQCCAy+dn8cTS0D7HlBcAMHN0IQBj7TkA1DmcQHdC9wfg2fUHeGrN3lCXiy8AdS3OfuO49eXN/PRfWyO2XXFCJdcvqh7CXyfEwElCF8PKZ/tbmXvn++wx+r9f3ngw9NjPX9vGNU+so9Pjw+cPRCzEPKowG4DCnCxevvYk/nrF8YCeqL0+P8t3H444z5aDbRxqczHD+ALY5+g/ob+5rTF0ITXopqUTGVWUE8NfKsTgSUIXw8oLRjfGh7v0BLzpYFvE49sbOqgxLkgWhNVPKcu3ATC3qoRRRTnY86yU5duoa+niuc8OsqOxI+J5nv20DqfXz/RRekJv6qcf3emJHPdeXZrLsuvmYZZiXCKJZAqbGDbW17WwfI8+yef37+3CbNKHCVYU2Dh+bDFmk4lXthxihzFksSishX7K5BHc/fkZLJjQvWZlZXEOK2scLNtUD8Avz5/Kbf+M7DI5YZydp9bup6WfIY4tTr3v/NoFVeTZsvjynEqprCiSTlroYtj4xpPraerobin/7t1dvLSxnsKcLH5x/jQumz0GgJ+/vg2AKRUFoX1NJhOLJpZGtJjHluRS3+bCH9DHq5+pynud84TKYixmUyhh9yWY8CeX5fOVE8dKMhcpIS10MewFl3gbYXSrBI2zH7nvetKI7kWKsy1mTCYT86tLsJhM3H/ViXy2uxF7npXinKx+W+itRsIvNsa8C5EKMSd0pdStwIWADfgj8B7wCPo8jI3ADZqmyXgtERfhxbYWTihh+e5mrpo7lr+t3hfaPqoom+Mri1hbpxfd6q+VrMJa8MFl4e69dCYA2Vnm0MiY4lwrL2w4yI/POAaLOfpzNhsJXxK6SKWYulyUUkuBhcAi4BRgHHAXcLumaScDJuCiOMUojmIeY/y309vdNrjz4hl8eONivrtkItNHFXLdwvEAmE0m/vuCaQN+7jnj7GRnmfnG/ComlfVd3rayWG/pH2rvu9xuo9EVVNbjV4IQyRRrH/rZwAbgeeBlYBkwB72VDvAKcMaQoxNHtU0HWll494d8stcR6vK47cxjyDKbyM7S37qPfPl4rl0wPnTMYFrIFrOJD763iG/2M078ohmjAI7Y7dLU4SbLbJKViURKxfruKwPGAxcAE4CXALOmacHfxW1AcX9PYrGYsNvz+tutj2PNMR+bSBLX4BwprrVr9boqK/a2MD9Lf6uOKSsY0N+hRg5sv4HEVWl0vfiyLKFtB1udEePLm7q8lBVkU1qS2IUshuP/YyodbXHFmtCbgK2aprkBTSnlRO92CSoEHP09ic8XwOHo7G+3qOz2vJiPTSSJa3COFFddkz788JGPa5VWwYMAABP8SURBVHjkY70Oy4QiW79/x3PXzKUkzzqkvzc8LrNXH2Ne19COY0QeW+vbuPLxtXxvyQRW1ThYNLGU1zYfZOnksoS/xsPx/zGVMjWu8vLCqNtjTegfAjcqpe4CRgP5wFtKqaWapr0LnAu8E+NzCwHAJ0aZ26CTquyUF2T3e9y4kty4xhHsxgmOZAmOqrnHqJO+oqYZE/AtmeIvUiymhK5p2jKl1BJgFXo//A3AbuABpZQN2AI8E7coxVHH4/Ozu6m7BfPEVXNSti5nsdEvHlzh6NWth3rts3BCKVVx/iIRYrBivoKjadotUTafMoRYhAg53Kknz/OOrcDnDzBhROr6Qa0WM3lWC61OLwdbnazY03tJuvICGd0iUk8uyYu0FKxBfurkMpYeU5biaKDImFz0gVFDZtaYItaHLTJdkC0fJZF6MvVfpKUmo4VemibjuotzrTS0u3lkZS0luVZ+cOqkiMcLJaGLNCAJXaSlYM2W0rz0mHnp8wdYVevgULubRRNLOXZUIb+9aDozR+sLZxRkyzJzIvWkWSHSQiAQoM3l5XCnh8v+uia0fWRh/6NakqG8wMaOxg5+feGxLJ08AtArOD69rg7Q+9mFSDVJ6CKlPD4/33nmM1xeP+vqWlkyaUTE4+mSKP/jHEWb09vr4uwVc8ayssbB5COUDhAiWSShi5Sqc3SxsqZ7DtqB1u6VgU6qsqcipKjK8m1R67QsmlDK8psWp80Xjzi6SUIXSXOw1cnDK2u5bPYYjikv4GCrk9+8uytin+DY83e/u5CcrOHRLy3JXKQLeSeKpHlnRxPPf3aQp9buJxAIcMnDq3l/eyMAT3x1DtlZZrz+ALlWM/m2rD5L1QohopOELpLmUJtefrah3UWby4vH113jfHxJLi6jRG6XR8roCxELSegiaYIJffnuZv65WZ8+f8vZitevn4/VYub7xiITZ0VZCk4I0T/pQxdJ8ZNlW3hDawjdv+udnQAcV1lMSZ5+sfHf5oxl6eQyKtJkqKIQw4200EXc+QMBOtzdiyq7vf5QMr92QVXEvlWlkcMAxxTnkCV950LERBK6iLs/fLCbpfcuD/WJr63Ty+Bet3A81y4YT55VH71y58XTGV185IWchRADJwldxN0LGw4CUG/0mT+yshbQC22ZTCbGGEn8hLH9LmolhBgE6UMXcRfsMjnY6qS8wMaavS1MLssP1TP/3eens76uVSoUChFn8okScRecaLO9oYP1dXqJ2fGl3Ys/jCrKiViPUwgRH9LlIuLOZtFb6He/t4sNB/SEftXccUc6RAgRB5LQxZA5Oj3sc3SF7gdXGwL4eE8zFQU2jh0VfVFbIUT8SEIXQ+IPBDjzTx9z+aOfAOD0+Ohw+/j24momGEMSR6TJIhVCZDpJ6GJIGtr1hShcXj/+QIBGY2GKsnwbiyeWAlBeIBOFhEgGuSgqhqSupbur5crHPmVbQwcAZQU2zlDlFGRncdZUmcovRDJIQhdDUuforl8eTOYAM0YVkWu1cM38qmiHCSESQBK6GJLtYUkc9MlCiyeWUpgjby0hkk0+dWJIth5qx2Yx4fYF+MdVJ3BMeUGqQxLiqCUJXcTM0eWhtrmL06eU87PzpqY6HCGOepLQRczO/OPHAJQXyLBEIdKBDFsUMeny+EK3ZViiEOlBErqIyf+t3R+6LQtSCJEeJKGLfmmH2pl75/u8qTXgDwTw+gPc98Hu0OPjS3KPcLQQIlmkD1306WCrk9++vZNdTfrQxFuXbeGkKjurah0R+42zS0IXIh1IQhdRvb2tgYdX7kU71B6xPTyZ3/eFmTR1uLFlyQ89IdKBJHTRy57Dnfzo5S1H3Odn5ynmjS9JUkRCiIGQhJ5igUCALo+fPJsltG3P4U6eWbefOePsNLS7uWz2aEym5Cyc/NHuw9z03MaIbSPybTQZRbfe+c5CWWlIiDQln8wUu+vdXTz5aR2/PH8qZ02tAOC/39jO2n0tPGWMJDmpys6v395BVUkuPz7jGAKBQNwTvD8Q4LHV+3hxwwHMJrjjXEV5fjaji7OpLM5l7p3vk2+zSDIXIo3JpzPFPtuvr+jz3o4mzppawYo9h1m7r4XpowrZdLANgOc3HGB1rYPVtQ52NXVyqM3Fc1+fizmOSX1LfXto5MpXTxrHudNGRjz+zNUnSn0WIdKcfEJTrM5Y6afB6NJ4dPU+8qwWfn/JDDw+P+fev5JXNh8K7b92XwsATR3uuE3ocXR6+Nrf14buTzEWcw433lisQgiRvmR4QgodanPR4vQC0NjuYtuhdtbUOvjavHEU51opK8jm5ImlNHd5mFCax6nHlIWO3Ru25NtQ3ffh7oj7U0fKcnFCDEdDaqErpSqAT4AzAS/wCBAANgI3aJrmH2qAmeyj3YcBWFBdwsd7mvmq0Uo+2+hLB/jekoks332Y6aML+cHSSVhM8Oa2Rv704R4euHx2zOfu8vh4dNVevnbSOHY2djCpLI8vzBpDQ7uLKpkoJMSwFHMLXSllBe4Hgk3Fu4DbNU07GTABFw09vMxR29zF7f/cgtOogRIIBHhrWwOjCrO55fTJTByRh9cfAGB0UXdXSvWIPP5w2XF8c+F4CnOy+J/PHUtFgY0Ot/48W+vbuP2fWzjQ6ux90ii8/gAdbi/Pf3aAh1bU8v0XNrGzsYPZlcV8YfYYrl88Ic5/uRAiWYbSQv8t8GfgVuP+HOA94/YrwFnA80d6AovFhN0eW9+sxWKO+dhE6iuu3763i9e2NrBoSjlXzK3iidW1rKxx8MMzpzCjegSv3bSET2qa2d/SRUlJZB/26T2eb/Ex5Xy8qwm7PY8HX9rMe9sbGV2Sx23nTYvYr93lxWo2kW21hOL60XMbeG5tHbcb5W7X1DqotOdw5cIJKXk9h9v/Y6pJXINztMUVU0JXSn0NaNA07TWlVDChmzRNCxi324Di/p7H5wvgcHTGEgJ2e17MxyZSX3EVGrMp/+OlzWze18KhNhdl+Ta+OHNkaP9JxdlMKs7u9+/KMYOj043D0Ulds/4Dqe5wZ6/j5t75PqqigMevPCEU13Nr6wBYW9Mc2u/eS2ZSmZeVktdzuP0/pprENTiZGld5efTrXLG20K8BAkqpM4DZwN+AirDHCwFHtAOPVsEuEoAnP9WTqqooiGk8eVFOFl0eP1vq20ITfpo73VH3DU7db3N6CAQCoe3/3FQPwFVzxzLWnjPoGIQQ6SemhK5p2pLgbaXUu8C3gN8opZZqmvYucC7wTjwCzBQtTg8AX59fxUMragEoybXG9FxFOfpxVz3ePdTwcKcnYh9/WPK+8rFP2XqonXOmVUTsU5Zv47tLJsYUgxAi/cRz2OLNwH8ppT4GbMAzcXzutHSozRXR6j2SAy1OplYU8K1F1Rw3pggg5qJWM0dH/tzKt1nY1dTJ+rqW0LZ2lzd0e6vRSn91iz6e/feXzACIKDcghBj+hjyxSNO0pWF3Txnq8w0XW+vbuPLxtXz35AlcddK4qPsEAgEeXFHL2n0trK1r5SsnjgXgyyeO5bOXNjNhRGwXRaaOLOSNby9ge0M7z64/wNfnV3HD0xv480d7+NMXZwGwvUEveXvTKRPZdLCNDq+ftk4PUyrymV9dwuUnVHLesRVHOo0QYpiRmaKD1OXxYUIfCw5w3we7uXT2aPJtvV/Kxg43f1leE7p//aJqAE47powXvjGXEXmxr8Vpz7Uyt6qEuVV6xcPPzRjF3z/Zh6PTgz3Pyrf+7zMASvKs/PcF03pdhLn51Ekxn1sIkZ4koQ/CxgOtXP2PdZwxpZwcq95dEgB+9eYOCmwWRhXloDV2csOiKn7z1s6IxZPnjbdHdLFUFsd38s55x1bwt9V7+f37uyLK2k4fJbM+hThaSEIfIK8/wJ3v7ATgzW0NzK8uYZw9h70OZ6hvOshm7p4FCvD01ScyKsHrbk4qy6coJ4tlm+pZtqkem8XEy9fNo3QIvwKEEMOL1HIZoIdX1LDxQBsnjtOH16/Y08y4klzuuXRGr33/uameE6vsAIwqzKa6NI8ca+IvQNrDRs3cfckMSeZCHGWkhT5Ab2gNzBtv5+7Pz+DyRz+hprmL4hwrC6pLeeP6BTy0spbSPCt//HAPAN9aOJ7x50/DnMSvzOsXVfPhrib+/fTJUfv0hRCZTVroA1DX0sWew13MGWcny2LmuoXjAVg8sRQAe56Vm0+dxGlh1RCPKS/AnmcNjRlPhjNUOXecO1WSuRBHKfnk9+Od7Y38ZNkWLGYTZ00tB+BMVY6qKOhVI3x8aR6f/OR0ujpcZMvCyUKIJJOs04+HVtQysjCbB740KzQyxWQy9bngQ1GuVZK5ECIlpIVuWLevhfo2F6X5Vrz+AAuqS2lzetEOtfPtxdXMNGZ3CiFEupKEjj5N/tqn1kdse/aauaHCV1PKC1IRlhBCDMpR3zfgDwRCKwXNrbJjNoofvru9kWfW7cdiNjF1pCR0IUT6O+pb6Bv2t1Jr1BT/42XHAXD+/Su49wN9nc2r5o5jRL6M5xZCpL+jvoVe16Iv3XbXxdND20aGzer86kljkx6TEELE4qhP6AdbXYDe3RJ04YxRAPzy/KlJHUcuhBBDkVFdLnuaOqkozB5QnW+Pz8/Xn1jHlvp2xtpzIqbmX3zcaM49dqQMPxRCDCsZk7Fe2VLPZY+s4Ucvb8bl9fe7/9b6drbU6ws//Oai6b0el2QuhBhuMiZrvb61AdCLZn3xr6txdHn6TOyBQIB/f2kzAA9dMZvJZflJi1MIIRIlI7pcPD4/q2sdzBpTRHlBNm9ua+DMP34MwPvfW0Su1UJLl4cHV9RyyXGjeXrdfpo63MwcXdRrOTchhBiuhn1CX777MD/911ZcXj/jS3O5/awpXLB7JDc9vxEArb6d6aMLOcNI8E9+WgfAzNFFPHjFLEwmU8piF0KIeBrWXS7tLi83PreRVqeX4pwsLp01BpPJxKKJpbz6rfmYTfDixoO8ZSwXF+6imSMxSzIXQmSQYZvQn1y9l1PvWw7AT8+ewhvfXsCxYcutjci3cfHM0SzbVM9P/7UVgBe/cRIVxrJwVSWxLdAshBDpalgm9C6Pj7vf2g7Av82p5HPTR0btOrns+DGh22dMKWNMcQ53Xjyd+dUlTJPp/EKIDDMs+9CXbaqnqcPNg5fPYlZlcZ/7TS7LZ/XNS2jscFOYrf+pU0cWcu+lM5MVqhBCJM2wTOjHjirktvOmHjGZhyuTWixCiKPAsEzo00cVsmjqSByOzlSHIoQQaWNY9qELIYToTRK6EEJkCEnoQgiRISShCyFEhpCELoQQGUISuhBCZAhJ6EIIkSEkoQshRIYwBQKBVJ6/AahJZQBCCDEMjQfKe25MdUIXQggRJ9LlIoQQGUISuhBCZAhJ6EIIkSEkoQshRIaQhC6EEBli2NVDV0qZgT8CswAX8A1N03akII55wP9qmrZUKTUZeAQIABuBGzRN8yul/hM4H/ACN2matiqB8ViBh4FqIBv4BbA5DeKyAA8ACvABVwOmVMcVFl8F8AlwpnHelMellFoLtBh3dwP3A783zv+6pmn/lYrPgVLqVuBCwGac+z1S//76GvA1424OMBtYSopfL+Pz+Cj659EHXEsS3l/DsYV+MZCjadoC4MfAnckOQCl1C/Ag+hsI4C7gdk3TTkZPVhcppU4ATgHmAZcDf0hwWF8BmowYzgXuS5O4Pgegadoi4D+MmNIhruCH7n6gy9iU8riUUjkAmqYtNf5dDfwZ+DdgMTDPiCmpnwOl1FJgIbAI/fUYRxq8XpqmPRJ8rdC/mL9HGrxewHlAlqZpC4GfAb8kCa/XcEzoi4FXATRNWwGcmIIYdgKXhN2fg95aAXgFOAM9ztc1TQtomlYLZCmlek0EiKOngZ+G3femQ1yapr0AXGfcHQ/Up0Ncht+if/j3G/fTIa5ZQJ5S6nWl1NtKqSVAtqZpOzVNCwCvAaeT/M/B2cAG4HngZWAZ6fF6AaCUOhGYDjxJerxe29D/djNQBHhIwus1HBN6Ed0/RwF8Sqmkdh1pmvYs+n9QkMl48wC0AcX0jjO4PVExtWua1qaUKgSeAW5Ph7iM2LxKqUeBe43YUh6X8VO9QdO018I2pzwuoBP9i+Zs4FvAX41tPc+f7M9BGXoSvMyI6++AOQ1er6CfAP9lnL81yvmT/Xq1o3e3bEXvcryHJLy/hmNCbwUKw+6bNU3zpioYgz/sdiHgoHecwe0Jo5QaB7wDPKZp2j/SJS4ATdO+CkxBf3PnpkFc1wBnKqXeRe93/RtQkQZxbQMeN1ps29A/7KUDiCvRn4Mm4DVN09yapmmAk8jEk8r3vR2YqmnaO0c4f7Jfr++jv15T0H91PYp+7aG/uIb0eg3HhP4Rev8USqn56D8DU22t0ccIev/1B+hxnq2UMiulqtDfQI2JCkApNRJ4HfiRpmkPp1FcVxoX00BvafqBNamOS9O0JZqmnWL0va4DrgJeSXVc6F80dwIopcYAeUCHUmqSUsqE3nIPxpXMz8GHwDlKKZMRVz7wVhq8XgBLgDcBNE1rBdxp8Ho1093yPgxYScLncdiNckHvwztTKbUc/cLC1SmOB+Bm4AGllA3YAjyjaZpPKfUB8DH6F+cNCY7hJ0AJ8FOlVLAv/UbgnhTH9RzwV6XU++hv6puMWFL9ekWTDv+PDwGPKKU+RB8NcQ36l+DfAQt6f+tKpdRqkvg50DRtmdGfv4ru12E3qX+9QB9BtSvsfrBLKGWvF/A74GHjtbChfz7XkODXS4pzCSFEhhiOXS5CCCGikIQuhBAZQhK6EEJkCEnoQgiRISShCyFEhpCELoQQGUISuhBCZAhJ6EIIkSH+HwheBiFssdeLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(running_mean(total_rewards,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to an extent, this succeeded! Obviously there is so much more to this kind of approach, but at a high level, this is the type of implementation neccessary to create an agent that acts based on a polcy gradient and understand whether or not it succeeded. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
