{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q Network applied to environment. I would suggest reading through this one first as I go over some of the shortcomings of this method, and how the policy gradient approach works better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from time import time # just to have timestamps in the files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize environment, device, and hyperparameters\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 1e-3\n",
    "target_update = 10\n",
    "memory_size = 10000\n",
    "lr = 0.001\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input the shape of observations, which is 4 for cartpole.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=self.state_space, out_features=24)   \n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.fc3 = nn.Linear(in_features=32,out_features=24)\n",
    "        self.out = nn.Linear(in_features=24, out_features=self.action_space)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = F.relu(self.fc3(t))\n",
    "\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "    \n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)\n",
    "    \n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "        \n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # convert current state to a tensor\n",
    "                state = torch.tensor(state).reshape(-1,4).float()\n",
    "                return policy_net(state).argmax().to(self.device) # exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device) #this one gets updated occasionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.tensor(batch.state)\n",
    "    t2 = torch.tensor(batch.action)\n",
    "    t3 = torch.tensor(batch.reward)\n",
    "    t4 = torch.tensor(batch.next_state)\n",
    "    t5 = torch.tensor(batch.done)\n",
    "#should I include a done bool?\n",
    "    return (t1.float(),t2.float(),t3.float(),t4.float(),t5.float())\n",
    "\n",
    "\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward','done')\n",
    ")\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1).long())\n",
    "    \n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states,dones):  #if done, need to include to set as 0   \n",
    "        done_mask = dones == 1 # if true, set to 0\n",
    "        values = target_net(next_states).max(dim=1)[0].detach()\n",
    "        values[done_mask] = 0.\n",
    "        return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "\n",
    "agent = Agent(strategy, env.action_space.n, device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "total_rewards = []\n",
    "RENDER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrappers.Monitor(env, '../videos/DQN/', force = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4ebad5ce0d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mcurrent_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mnext_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtarget_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnext_q_values\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-d0bd7e48219f>\u001b[0m in \u001b[0;36mget_current\u001b[0;34m(policy_net, states, actions)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_current\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a8f194c32bd4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset() #initial state\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    while not done:\n",
    "        if RENDER:\n",
    "            #every 5 episodes, render progress. \n",
    "            if episode % 5 == 0:\n",
    "                env.render()\n",
    "        timestep += 1\n",
    "        action = agent.select_action(state, policy_net).item()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(Experience(state, action, next_state, reward,done))\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        #once enough replays are collected so it is possible to learn. \n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states, dones = extract_tensors(experiences)\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states,dones)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    total_rewards.append(timestep)\n",
    "    \n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    \"\"\"\n",
    "    Useful function to smooth rewards over episodes. \n",
    "    \"\"\"\n",
    "    cumsum = np.cumsum(np\n",
    "                       .insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x132ccafd0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(running_mean(total_rewards,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So something that suprised me (and you too) is that this method does far better in terms of accumulating reward and staying alive in this task as compared to the policy gradient approach. \n",
    "\n",
    "\n",
    "One first disclaimer in this is that I did basically no hyperparameter tuning for the policy gradient, whereas the DQN I spent a ton of time tuning at first and more or less wanted to gain a basic understanding and implementation of policy gradients. All this to say I think if I spent enough time, I could get the policy gradient to work a lot quicker. \n",
    "\n",
    "\n",
    "Additionally, one of the shortcomings of the DQN is that I basically had to hard code in the action space. For a network which needs to predict an infinite number of actions, this can be problematic. Policy gradients resolve this by computing probability distributions for a set of actions which is a lot easier to look into. \n",
    "\n",
    "And lastly, the DQN predicts deterministic values, and definitely tells you to go left or right (or whatever the action is). Policy networks will output a probability distribution which you draw from, so there is already some inherent stochasity involved and no longer requires this hard coded in exploration via epsilon greedy. \n",
    "\n",
    "\n",
    "Of course there are caveats to Policy Gradients too, but I'll leave that for another time... \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
